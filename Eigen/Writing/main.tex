
\documentclass[a4paper,12pt]{article}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{txfonts}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{gensymb}
\usepackage{comment}
\usepackage[breaklinks=true]{hyperref}
\usepackage{tkz-euclide} 
\usepackage{listings}
\usepackage{gvv}                                        
%\def\inputGnumericTable{}                                 
\usepackage[latin1]{inputenc}     
\usepackage{xparse}
\usepackage{color}                                            
\usepackage{array}                                            
\usepackage{longtable}                                       
\usepackage{calc}                                             
\usepackage{multirow}
\usepackage{multicol}
\usepackage{hhline}                                           
\usepackage{ifthen}                                           
\usepackage{lscape}
\usepackage{tabularx}
\usepackage{array}
\usepackage{float}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}[section]
\newtheorem{definition}[problem]{Definition}
\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}
\theoremstyle{remark}
\newtheorem{rem}{Remark}

\begin{document}
\title{Software Assignment}
\author{EE24Btech11024 - G. Abhimanyu Koushik}
\maketitle
\renewcommand{\thefigure}{\theenumi}
\renewcommand{\thetable}{\theenumi}

\section*{Introduction}
An $N\times N$ matrix $A$ is said to have an eigenvector $\vec{x}$ and corresponding eigenvalue $\lambda$ if
\begin{align}
	A\cdot\vec{x} = \lambda\vec{x}
\end{align}
Any multiple of an eigenvector is also an eigenvector, but we will not be considering such multiples as disticnt vectors.\\
Evidently equation \brak{1} holds only if
\begin{align}
	\det\abs{A-\lambda I} = 0
\end{align}
which, if expanded out, is an $N^{\text{th}}$ degree polynomial equation in $\lambda$ whose roots are eigenvalues. Root searching in the characteristic equation \brak{2} is a very poor computational method of finding eigenvalues.\\
The above two equations show that there is a corresponding eigenvector or every eigenvalue: If $\lambda$ is set to an eigenvalue then the matrix $A-\lambda I$ is singular so it has atleast one non-zero vector in its nullspace.\\
A matrix is called symmetric if it is equal to its transpose.
\begin{align}
	A = A^\top
\end{align}
A matrix is called orthogonal if its inverse is equal to its transpose.
\begin{align}
	A^\top\cdot A = A\cdot A^\top = I
\end{align}
Multiply two orthogonal matrices gives another orthogonal matrix.
The eigenvalues of symmetric values are all real. The eigenvalues of a non-symmetric matrix can be complex. 
\subsection*{Diagonalization of a Matrx}
A matrix is said to undergo Similarity transformation if it undergoes the following process.
\begin{align}
	A \implies Z^{-1}\cdot A\cdot Z
\end{align}
For some matrix $Z$. When a matrix undergoes similarity transform, the eigenvalues of the matrix remain unchanged.
\begin{align}
	\det\abs{Z^{-1}\cdot A\cdot Z - \lambda I} &= \det\abs{Z^{-1}\cdot A\cdot Z - \lambda Z^{-1}\cdot I\cdot Z}\\
		&= \det\abs{Z^{-1}\brak{A - \lambda I }Z}\\
		&= \det\abs{Z^{-1}}\det\abs{A - \lambda I}\det\abs{Z}\\
		&= \det\abs{A - \lambda I} 
\end{align}
If a matrix undergoes similarity transform, the properties like symmetry, normality, unitary remain preserved..
For a matrix $A$, let the eigen values be $\lambda_1, \lambda_2,\dots,\lambda_N$, then we can write the following equation.
\begin{align}
	X\cdot A= \text{diag}\brak{\lambda_1, \lambda_2,\dots,\lambda_N}\cdot X\\
\end{align}
where RHS is a diagonal matrix with entries $\lambda_1, \lambda_2,\dots,\lambda_N$, if we multiply with $X^{-1}$ on both side we will get
\begin{align}
	X \cdot A \cdot X^{-1} = \text{diag}\brak{\lambda_1, \lambda_2,\dots,\lambda_N}
\end{align}
This is the basis for Jacobian Transformation

\section{Jacobian Transformation}
The Jacobian transformation consists of a sequence of orthogonal similarity transformation \brak{X \text{ is orthogonal}} to tranform matrix $A$ into a diagonal matrix. Each orthogonal transformation is designed to annilate one off-diangonal element. Successive tranformations might undo previously set zeroes but the off-diagonal elements nevertheless get smaller and smaller, until the matrix is diagonal to machine precision.

\subsection{Mathematical Background}
Let $A$ be a symmetric matrix. The Jacobian method aims to diagonalize $A$ into:
\begin{align}
A = V \Lambda V^\top
\end{align}
where $\Lambda$ is a diagonal matrix of eigenvalues, and $V$ is an orthogonal matrix of eigenvectors.
The rotation matrix $P_{pq}$ is defined as:
\begin{align}
P_{pq} = \myvec{
1 & \cdots & 0 & 0 & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \reflectbox{$\ddots$} & \vdots \\
0 & \cdots & c & s & \cdots & 0\\
0 & \cdots & -s & c & \cdots & 0\\
\vdots & \reflectbox{$\ddots$} & \vdots & \vdots & \ddots & \vdots\\
0 & \cdots & 0 & 0 & \cdots & 1
}
\end{align}
Here all diagonal elements are 1 except for the two elements $c$ in rows \brak{\text{and columns}} $p$ and $q$. All off-diagonal elements are 0 except for $s$ and $-s$. The numbers $c$ and $s$ are the cosine and sine of rotation angle $\phi$, so $c^2+s^2=1$.\\
A plane rotation such as $P_{pq}$ in equation \brak{14} is used to transform matrix $A$ according to
\begin{align}
	A^\prime = P_{pq}^\top A P_{pq}
\end{align} 
Now, $P_{pq}^\top A$ changes only rows $p$ and $q$ of $A$., while $AP_{pq}$ change only the columns $p$ and $q$. Thus the changed elements of $A$ in equation \brak{15} are only in $p$ and $q$ rows and columns indicated as below
\begin{align}
	A^\prime =\myvec{
 & \cdots & a_{1p}^\prime & \cdots & a_{1q}^\prime & \cdots & \\
\vdots &  & \vdots & & \vdots & & \vdots \\
a_{p1}^\prime & \cdots & a_{pp}^\prime & \cdots & a_{pq}^\prime & \cdots & a_{pn}^\prime\\
\vdots & & \vdots & & \vdots & & \vdots\\
a_{q1}^\prime & \cdots & a_{qp}^\prime & \cdots & a_{qq}^\prime & \cdots & a_{qn}^\prime\\
\vdots & & \vdots & & \vdots & & \vdots \\
& \cdots & a_{np}^\prime & \cdots & a_{nq}^\prime & \cdots& }
\end{align}
Multiplying equation \brak{15} and using the symmetry of $A$ will get us the equations
\begin{align}
a_{rp}^\prime &= ca_{rp} - sa_{rq}\\
a_{rq}^\prime &= ca_{rq} + sa_{rp}\\
a_{pp}^\prime &= c^2a_{pp} + s^2a_{qq} - 2sca_{pq}\\
a_{qq}^\prime &= c^2a_{pp} + s^2a_{qq} + 2sca_{pq}\\
a_{pq}^\prime &= \brak{c^2-s^2}a_{pq} +sc\brak{a_{pp}-a_{qq}} 
\end{align}
for $r \neq s$. Accordingly, if we set $a_{pq}^\prime = 0$, equation \brak{21} gives the following expression
\begin{align}
	\theta = \cot{2\phi} = \frac{c^2-s^2}{2sc} = \frac{a_{qq}-a_{pp}}{2a_{pq}}
\end{align}
If we let $t = \frac{s}{c}$, the definition of $\theta$ can be written as 
\begin{align}
	t^2+2t\theta - 1=0
\end{align}
We get $t$ as the following value if we consider the smaller root
\begin{align}
	t = \frac{sgn\brak{\theta}}{\abs{\theta}+\sqrt{\theta^2+1}}
\end{align}
It now follows that 
\begin{align}
	c&=\frac{1}{\sqrt{t^2+1}}\\
	s&=\frac{t}{\sqrt{t^2+1}}
\end{align}
Once we substitute the $c$ and $s$ in the matrix and multiply as given in equation \brak{15}, the $a_{pq}^\prime$ gets nulled. We not do the same thing for all the other elements as well. Since the matrix is symmetric, there is no need to iterate through every off-diagonal element. Just going through the lower triangular indexed elements would be enough as it will automatically null upper triangular ones as well.\\
One can see the convergence of Jacobi method by considering sum of square of off-diagonal elements.
\begin{align}
	S = \sum_{r\neq s}\abs{a_{rs}}^2
\end{align}
The new sum after an iteration will be
\begin{align}
	S^\prime = S - 2\abs{a_{pq}}^2 
\end{align}
Since the sequence is monotonically decreasing and is bounded below by 0, it coverges to 0.\\
Eventually, one obtains a matrix $D$ which is diagonal to machine precision. The elements of the matrix $D$ will be the eigenvalues of $A$ since
\begin{align}
	D = V^\top A V
\end{align}
where
\begin{align}
	V = P_1\cdot P_2\cdot P_3 \cdots 
\end{align}
One set of $\frac{n\brak{n-1}}{2}$ set of Jacobi rotations is called a sweep, nulling every lower triangular element once. We will implement 8 sweeps so as to make sure the elements are nulled completely and then stop.
\subsection{Code}
Below is the implementation of the Jacobian transformation in C:
\begin{verbatim}
double **jacobian(double **A, int dim) {
    double **jacobian = copyMat(A, dim, dim);
    double threshold = 1e-10;
\end{verbatim}
Initialising jacobian matrix \brak{\text{The diagonal matrix}}, and keep a threshold.
\begin{verbatim}
    for (int sweep = 0; sweep < 8; sweep++) {
        for (int p = 1; p < dim; p++) {
            for (int q = 0; q < p; q++) {
                if (fabs(jacobian[p][q]) > threshold) {
\end{verbatim}
Applying jacobian transform 8 times, and iterating through each lower triangular element and applying transformation if its absolute value is greater than threshold (basically greater than 0).
\begin{verbatim}
                    double theta = (jacobian[q][q] - jacobian[p][p]) / (2 * jacobian[p][q]);
                    double sgn = (theta != 0) ? (theta > 0 ? 1 : -1) : 0;
                    double t = sgn / (fabs(theta) + sqrt(theta * theta + 1));
                    double c = 1 / sqrt(t * t + 1);
                    double s = t / sqrt(t * t + 1);
\end{verbatim}
Calculatint the values of $c$,$s$,$t$ and $\theta$.
\begin{verbatim}
                    double **Ppq = identity(dim);
                    Ppq[p][p] = c;
                    Ppq[p][q] = s;
                    Ppq[q][p] = -s;
                    Ppq[q][q] = c;

\end{verbatim}
Forming the matrix $P_{pq}$.
\begin{verbatim}
                    jacobian = Matmul(jacobian, Ppq, dim, dim, dim);
                    double **PpqT = transposeMat(Ppq, dim, dim);
                    jacobian = Matmul(PpqT, jacobian, dim, dim, dim);
\end{verbatim}
Assigning new $A$ as $P_{pq}^\top A P_{pq}$ 
\begin{verbatim}
                    freeMat(Ppq, dim);
                    freeMat(PpqT, dim);
                }
\end{verbatim}
Free the matrices
\begin{verbatim}
                else{
                	jacobian[p][q] = 0;
                }
            }
        }
    }
    return jacobian;
}
\end{verbatim}
Assigning the values 0 if they are less than threshold, and return the diagonal matrix

\section{Hessenberg Reduction}
Jacobian is a very inefficient way of finding the eigenvalues as its time complexity is $O\brak{n^4}$ and its use is very limited as it can only be used to find the eigenvalues of real and symmetric matrices. For finding the eigen values of a non-symmetric, the best way is to reduce the matrix to a suitable form and then apply QR decomposition algorithm to that, which the will be explained later. This is suitable for turns out to be Hessenberg form. Turning a matrix into hessenberg form is a process of time complexity $O\brak{n^3}$, and then applying QR decomposition on hessenberg is of time complexity $O\brak{n^2}$. Hence the actual time complexity of order $O\brak{n^3}$.

\subsection{Mathematical Background}
We say a matrix $A$ is in hessenberg form if it is in form shown below
\begin{align}
H = 
\myvec{
\times & \times & \times & \cdots & \times \\
\times & \times & \times & \cdots & \times \\
0      & \times & \times & \cdots & \times \\
0      & 0      & \times & \cdots & \times \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0      & 0      & 0      & \cdots & \times
}
\end{align}
We will use householder method to reduce any matrix into hessenberg form.\\
It reduces an $n\times n$ matrix to hessenberg form by $n-2$ orthogonal trasformations. Each transformations annihilates the required part of a whole column at a time rather than element wise elimination. The basic ingredient for a house holder matrix is $P$ which is in the form
\begin{align}
	P = I-2\vec{w}\vec{w}^\top
\end{align}
where w is a vector with $\abs{w}^2=1$. The matrix $P$ is orthogonal as
\begin{align}
P^2 &=\brak{I - 2\vec{w}\vec{w}^\top}\cdot\brak{I - 2\vec{w}\vec{w}^\top}\\
    &=I-4\vec{w}\vec{w}^\top + 4\vec{w}\cdot\brak{\vec{w}^\top\vec{w}^\top}\cdot\vec{w}^\top\\
    &=I
\end{align}
Therefore, $P=P^{-1}$ but $P = P^\top$, so $P = P^\top$ \\
We can rewrite $P$ as
\begin{align}
	P = I - \frac{\vec{u}\vec{u}^\top}{H}
\end{align}
where the scalar $H$ is
\begin{align}
H = \frac{1}{2}\abs{\vec{u}}^2
\end{align}
Where $\vec{u}$ can be any vector. Suppose $\vec{x}$ is the vector composed of the first column of $A$. Take
\begin{align}
	\vec{u} = \vec{x} \mp \abs{\vec{x}}\vec{e}_1
\end{align}
Where $\vec{e}_1 = \myvec{1 & 0 & \dots}^\top$, we will take the choice of sign later. Then
\begin{align}
	P\cdot\vec{x} &= \vec{x} - \frac{\vec{u}}{H}\cdot\brak{\vec{u}\mp \abs{\vec{x}}\vec{e}_1}^\top\cdot \vec{x}\\
	              &= \vec{x} - \frac{2\vec{u}\brak{\abs{x}^2\mp \abs{x}x_1}}{2\abs{x}^2\mp \abs{x}x_1}\\
	              &= \vec{x} - \vec{u}\\
	              &= \mp\abs{\vec{x}}\vec{e}_1
\end{align}
To reduce a matrix $A$ into Hessenberg form, we choose vector $\vec{x}$ for the first householder matrix to be lower $n-1$ elements of the first column, then the lower $n-2$ elements will be zeroed.
\begin{align}
P_1\cdot A &= \myvec{
1 & 0 & 0 & \cdots & 0 \\
0 & p_{11} & p_{12} & \cdots & p_{1n} \\
0      & p_{21} & p_{22} & \cdots & p_{2n} \\
0      & p_{31} & p_{32} & \cdots & p_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0      & p_{n1} & p_{n2} & \cdots & p_{nn}
}\myvec{
a_{00} & \times & \times & \cdots & \times \\
a_{10} & \times & \times & \cdots & \times \\
a_{20} & \times & \times & \cdots & \times \\
a_{30} & \times & \times & \cdots & \times \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
a_{n0} & \times & \times & \cdots & \times
}\\
&=\myvec{
a_{00}^\prime & \times & \times & \cdots & \times \\
0 & \times & \times & \cdots & \times \\
0      & \times & \times & \cdots & \times \\
0      & \times   & \times & \cdots & \times \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0      & \times      & \times      & \cdots & \times
}
\end{align}
Now if we multiply the matrix $P_1A$ with $P_1$, the eigenvalues will be conserved as it is a similarity transformation.\\
Now we choose the vector $\vec{x}$ for the householder matrix to be the bottom $n-2$ elements of the second column, and from it construct the $P_2$
\begin{align}
P_2 = \myvec{
1 & 0 & 0 & \cdots & 0 \\
0 & 1 & 0 & \cdots & 0 \\
0 & 0 & p_{22} & \cdots & p_{2n} \\
0 & 0 & p_{32} & \cdots & p_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & p_{n2} & \cdots & p_{nn}
}
\end{align}
Now if do similarity transform $PAP$, we will zero out the $n-3$ elements in second column.
If we continue this pattern we will get the hessenberg form of a the matrix $A$.
\subsection{Code}
\begin{verbatim}
double complex **makehessberg(double complex **A, int dim) {
    double complex **toreturn = copyMat(A, dim, dim);
\end{verbatim}
Copying inputted matrix to toreturn;
\begin{verbatim}
    for (int k = 0; k < dim - 2; k++) {
        int u_dim = dim - k - 1;
        double complex **u = createMat(u_dim, 1);
        for (int i = 0; i < u_dim; i++) {
            u[i][0] = toreturn[k + 1 + i][k];
        }
\end{verbatim}
Making a loop for $n-2$ iteration and initializing $\vec{u}$ to the required subcolumn vector of $A$ 
\begin{verbatim}        
        double complex rho;
        if(cabs(u[0][0])!=0){
        	rho = - u[0][0] / cabs(u[0][0]);
        }
        else{
        	rho = 1;
        }
        u[0][0] += -1 * rho * Matnorm(u, u_dim);
\end{verbatim}
Substituting $\vec{u}=\vec{x}-\abs{\vec{x}}\vec{e}_1$
\begin{verbatim}
        double norm_u = Matnorm(u, u_dim);
        if (norm_u != 0.0) {
         	   for (int i = 0; i < u_dim; i++) {
                u[i][0] /= norm_u;
            }
        }
        else{
        	continue;
        }
\end{verbatim}
Making $\vec{u}$ an unit vector and if norm of $\vec{u}=0$, skipping the process as the column is already in required form
\begin{verbatim}
        double complex **Hk = identity(dim);
        double complex **u_transpose = transposeMat(u, u_dim, 1);
        double complex **u_ut = Matmul(u, u_transpose, u_dim, 1, u_dim);

        for (int i = 0; i < u_dim; i++) {
            for (int j = 0; j < u_dim; j++) {
                Hk[k + 1 + i][k + 1 + j] -= 2 * u_ut[i][j];
            }
        }
\end{verbatim}
Forming the Householder matrix
\begin{verbatim}
        toreturn = Matmul(Hk, toreturn, dim, dim, dim);
        toreturn = Matmul(toreturn, Hk, dim, dim, dim);
\end{verbatim}
Using similarity trasformation with Householder matrix to $A$.
\begin{verbatim}
        freeMat(u, u_dim);
        freeMat(u_transpose, 1);
        freeMat(u_ut, u_dim);
        freeMat(Hk, dim);
    }
    return toreturn;
}
\end{verbatim}
Freeing the matrices and returning the hessenberg form of $A$.
\section{QR Decomposition Algorithm}
\subsection{Mathematical Background}
In this algorithm, we decompose matrix given in Hessenberg form to two matrices $Q$ and $R$ such that $Q$ is an orthogonal matrix and $R$ is an upper triangular matrix. Then we assign the new matrix $A^\prime$ to be $A^\prime = RQ$, and we do this iteratively. Theoritically, as the number of iterations go to infinite, the matrix $A^\prime$ will converge to an upper triangular matrix whose diagonal elements are the eigenvalues of $A$. There will be a minor problem in this method when the entries are real while the eigenvalues are complex, we will solve this issue shortly. The eigenvalues of the matrix $A$ will not change because of the following
\begin{align}
	A &= QR\\
	R &= Q^\top A\\
	A^\prime &= RQ\\
	A^\prime &= Q^\top AQ
\end{align}
As the matrix $A$ is undergoing similarity transformation, the eigenvalues will not change.\\
The rate of covergence of $A$ depends on the ratio of absolute values of the eigenvalues. That is, if the eigenvalues are $\abs{\lambda_1} \geq \abs{\lambda_2} \geq \abs{\lambda_3} \dots \geq \abs{\lambda_n}$ then, the elements of $A_k$ below the diagonal to converge to zero like
\begin{align}
    \abs{a_{ij}^{\brak{k}}} = O\brak{\abs{\frac{\lambda_i}{\lambda_j}}^k}\text{    } i > j
\end{align}

\subsection{Givens Rotations}
The QR decomposition is implemented using the Givens rotation technique. This approach is robust and numerically stable, making it ideal for QR decomposition, especially in iterative methods like eigenvalue computations. It is every similar to Jacobian Transformation. We define a rotation matrix $P$, to zero out the elements which are non-diagonal, since the matrix which we are dealing is a Hessenberg matrix, we need to zero out the elements which are just below the diagonal elements. This shows the significance of Hessenberg form. Initially we zeroed complete columns at once. If we were to apply givens to each and every element, it will take $\frac{n\brak{n-1}}{2}$ orthogonal rotations. Now, computing $P$ and the new $A$, costs in order $O\brak{n^2}$. The whole process together would be in cost of order $O\brak{n^4}$. But now since we only need to do $n-1$ orthogonal rotations, the order becomes $O\brak{n^3}$. The rotation matrix $P$ is defined as 
\begin{align}
P = \myvec{
1 & \cdots & 0 & 0 & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \reflectbox{$\ddots$} & \vdots \\
0 & \cdots & c & s & \cdots & 0\\
0 & \cdots & -s & c & \cdots & 0\\
\vdots & \reflectbox{$\ddots$} & \vdots & \vdots & \ddots & \vdots\\
0 & \cdots & 0 & 0 & \cdots & 1
}
\end{align}
Where the value of $c$ and $s$ are
\begin{align}
	c = \frac{\overline{x_{i,i}}}{\sqrt{\abs{x_{i,i}}^2+\abs{x_{i,i+1}}^2}}
\end{align}
\subsubsection{Givens Rotation Matrix}
A Givens rotation matrix is a 2x2 orthogonal matrix used to zero out specific elements of a matrix through rotation. The Givens rotation matrix \( G \) is defined as:

\[
G(\theta) = \begin{pmatrix}
c & s \\
-s & c
\end{pmatrix}
\]

where \( c = \cos(\theta) \) and \( s = \sin(\theta) \) for an angle \( \theta \), chosen such that the element below the diagonal in the matrix \( A \) is eliminated.

\subsubsection{Steps in the QR Decomposition Algorithm}
The algorithm proceeds as follows:
\begin{enumerate}
    \item Initialize \( A \) as the input matrix, \( Q = I \) (identity matrix), and \( R = A \).
    \item Iterate through the columns of \( A \), and for each pair of elements in the column, compute the necessary Givens rotation matrix to zero out the subdiagonal element.
    \item Apply the Givens rotation to both \( A \) and \( Q \) in each iteration.
    \item The resulting matrix \( R \) will be upper triangular, and the product of all the Givens rotation matrices accumulated in \( Q \) will form the orthogonal matrix \( Q \).
\end{enumerate}

The implementation in \texttt{qr.c} handles the iterative application of the Givens rotations and the accumulation of the orthogonal matrix \( Q \).

\subsection{Rayleigh Quotient Iteration (rayleigh.c)}
Rayleigh Quotient iteration is an iterative method used to find eigenvalues of a matrix, which can be especially useful when computing the dominant eigenvalue. The method uses the Rayleigh quotient as an approximation for the eigenvalue:

\[
\lambda = \frac{v^T A v}{v^T v}
\]

where \( v \) is the current vector approximation to the eigenvector. The basic idea of the Rayleigh Quotient iteration is to refine the approximation by solving for the eigenvector corresponding to the largest eigenvalue of a shifted matrix.

\subsubsection{Steps in the Rayleigh Quotient Iteration Algorithm}
The steps for Rayleigh Quotient iteration are as follows:
\begin{enumerate}
    \item Start with an initial guess for the eigenvector \( v_0 \).
    \item Compute the Rayleigh quotient \( \lambda_0 = \frac{v_0^T A v_0}{v_0^T v_0} \).
    \item Shift the matrix \( A \) by subtracting \( \lambda_0 I \) from it, forming the shifted matrix \( A - \lambda_0 I \).
    \item Solve the linear system \( (A - \lambda_0 I) v_1 = v_0 \) for the new vector \( v_1 \).
    \item Normalize \( v_1 \) and repeat the process, refining both the eigenvalue and eigenvector with each iteration.
\end{enumerate}

The implementation of this method in \texttt{rayleigh.c} allows for the iterative calculation of eigenvalues with high accuracy.

\subsection{Code Explanation}
The following sections explain the contents of the \texttt{qr.c} and \texttt{rayleigh.c} files.

\subsubsection{qr.c}
The \texttt{qr.c} file contains the implementation for performing QR decomposition using Givens rotations. The main functions in the file are:

\begin{itemize}
    \item \texttt{givens\_rotation(double* A, int m, int n, int i, int j)}: This function computes the Givens rotation matrix to zero out the element \( A[i,j] \) in the matrix \( A \).
    \item \texttt{qr\_decomposition(double* A, double* Q, double* R, int m, int n)}: This function applies the Givens rotations iteratively to compute the QR decomposition of matrix \( A \). It updates matrices \( Q \) and \( R \) accordingly.
\end{itemize}

\subsubsection{rayleigh.c}
The \texttt{rayleigh.c} file implements the Rayleigh Quotient iteration method for finding eigenvalues. The main functions in the file are:

\begin{itemize}
    \item \texttt{rayleigh\_quotient(double* A, double* v, int n)}: This function computes the Rayleigh quotient \( \lambda = \frac{v^T A v}{v^T v} \).
    \item \texttt{rayleigh\_iteration(double* A, double* v, int n, int max\_iter)}: This function performs the Rayleigh Quotient iteration, refining the eigenvalue and eigenvector with each iteration.
\end{itemize}

The code in \texttt{rayleigh.c} iteratively refines both the eigenvalue and eigenvector using the Rayleigh quotient and shifts to converge to the true eigenvalue.

\subsection{Conclusion}
The QR decomposition algorithm, implemented in \texttt{qr.c} using Givens rotations, is a stable and efficient method for decomposing matrices. The Rayleigh Quotient iteration, implemented in \texttt{rayleigh.c}, provides an iterative technique for refining eigenvalue approximations. Both algorithms play crucial roles in numerical linear algebra and are essential tools for solving systems of linear equations and finding eigenvalues.

\section{QR with Rayleigh Quotient method}
\textit{(Content derived from qr.c, explaining its importance for eigenvalue convergence.)}

\section*{References}
\begin{enumerate}
    \item W. H. Press et al., \textit{Numerical Recipes in C: The Art of Scientific Computing}.
    \item P. Arbenz, \textit{Lecture Notes on Numerical Linear Algebra}. 
        \href{https://people.inf.ethz.ch/arbenz/ewp/Lnotes/chapter4.pdf}{Link}.
\end{enumerate}

\end{document}
